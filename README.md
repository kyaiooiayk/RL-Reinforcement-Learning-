# RL - Reinforcemen Learning
***

## What is wrong with RL back then and now?
- Imagine a cruel experiment, only as a thought experiment in our imagination, of strapping a new born baby behind the wheel of a car, and trying to teach the baby to drive. Would this ever work? Not with a human baby. Our motor control system is not developed, our visual system still is evolving, and last but not least, we have no clue what we are supposed to be doing. And yet, this is what modern deep RL is attempting to do, trying to teach “infant RL” systems tasks for which they are fundamentally unequipped to handle. The results from the hundreds of published papers is not surprising. It takes forever for RL systems to learn in this tabula rasa mode. It will not scale to the real world, but it will only work in simulation.
- So, in a fundamental sense, RL is no more advanced today in 2017 than back in my early attempt in 1992 to program the first real robot with RL. Why has the field not fundamentally advanced in solving this problem? The reasons have much to do with the emphasis being on “performance” on a single task, at whatever the training cost. Most deep RL systems still take millions of simulated steps, because they all start with tabula rasa — a blank slate. Humans, like my study above of riding a bicycle — never begin any task with a blank slate.
[Reference](https://www.quora.com/What-advantage-does-deep-reinforcement-learning-bring-to-the-table-Or-is-it-redundant-and-deep-learning-and-reinforcement-learning-perform-equally-good-separately-Pardon-my-ignorance-and-answer-like-you-were)
***

## Available tutorials
- Tutorials focused on KERAS (TS) framework
- Tutorials focused on PyTorch (PT) framework
- Tutorials focused on other framework (not DL)
***
